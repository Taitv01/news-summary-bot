#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import os
import logging
import time
from scraper import NewsScraper
from content_extractor import extract_content
from telegram_sender import send_telegram_message, escape_markdown_v2
from summarizer import summarize_with_gemini

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_rss_sources():
    """Táº£i cÃ¡c nguá»“n RSS tá»« cáº¥u hÃ¬nh."""
    return {
        'VnExpress Má»›i nháº¥t': 'https://vnexpress.net/rss/tin-moi-nhat.rss',
        'VnExpress Kinh doanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'Vietstock Chá»©ng khoÃ¡n': 'https://vietstock.vn/rss/chung-khoan.rss',
        'Lao Äá»™ng': 'https://laodong.vn/rss/tin-moi-nhat.rss'
    }

def load_processed_links():
    """Táº£i danh sÃ¡ch cÃ¡c link Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½."""
    file_path = 'data/processed_links.json'
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except json.JSONDecodeError:
            logger.warning(f"Lá»—i Ä‘á»c file {file_path}. Báº¯t Ä‘áº§u vá»›i danh sÃ¡ch rá»—ng.")
            return set()
    return set()

def save_processed_links(links):
    """LÆ°u danh sÃ¡ch cÃ¡c link Ä‘Ã£ xá»­ lÃ½."""
    os.makedirs('data', exist_ok=True)
    with open('data/processed_links.json', 'w', encoding='utf-8') as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)

def process_news():
    """Quy trÃ¬nh xá»­ lÃ½ tin tá»©c chÃ­nh."""
    logger.info("--- Bot tÃ³m táº¯t tin tá»©c báº¯t Ä‘áº§u cháº¡y ---")
    scraper = NewsScraper()
    
    try:
        processed_links = load_processed_links()
        logger.info(f"ÄÃ£ táº£i {len(processed_links)} links Ä‘Ã£ xá»­ lÃ½.")
        
        rss_sources = load_rss_sources()
        logger.info("Äang kiá»ƒm tra tin tá»©c tá»« táº¥t cáº£ cÃ¡c nguá»“n RSS...")
        
        new_articles = []
        for source_name, rss_url in rss_sources.items():
            logger.info(f"-> Äang láº¥y tá»«: {source_name}")
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                if entry.link not in processed_links:
                    new_articles.append({'title': entry.title, 'link': entry.link})
        
        if not new_articles:
            logger.info("KhÃ´ng cÃ³ bÃ i bÃ¡o má»›i.")
            return

        logger.info(f"PhÃ¡t hiá»‡n {len(new_articles)} bÃ i bÃ¡o má»›i. Báº¯t Ä‘áº§u xá»­ lÃ½...")
        
        # VÃ’NG Láº¶P 1: THU THáº¬P VÃ€ TÃ“M Táº®T
        articles_to_send = []
        for article in new_articles:
            logger.info(f"Äang xá»­ lÃ½: {article['title'][:40]}...")
            response = scraper.get_content_with_retry(article['link'])
            
            if response and response.status_code == 200:
                content = extract_content(response.text, article['link'])
                if content:
                    logger.info("-> Láº¥y ná»™i dung thÃ nh cÃ´ng, Ä‘ang gá»­i Ä‘i tÃ³m táº¯t...")
                    summary = summarize_with_gemini(content)
                    article['summary'] = summary
                    articles_to_send.append(article)
                    processed_links.add(article['link'])
                else:
                    logger.warning("-> KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c ná»™i dung.")
            else:
                logger.warning("-> KhÃ´ng táº£i Ä‘Æ°á»£c trang.")

        # VÃ’NG Láº¶P 2: Gá»¬I Káº¾T QUáº¢ Tá»šI TELEGRAM
        if articles_to_send:
            logger.info(f"ÄÃ£ xá»­ lÃ½ xong. Chuáº©n bá»‹ gá»­i {len(articles_to_send)} tin nháº¯n Ä‘áº¿n Telegram.")
            for article in articles_to_send:
                # Sá»­ dá»¥ng hÃ m escape chuyÃªn dá»¥ng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
                title = escape_markdown_v2(article['title'])
                summary_text = escape_markdown_v2(article.get('summary', 'KhÃ´ng cÃ³ tÃ³m táº¯t'))
                link = article['link'] # Link khÃ´ng cáº§n escape

                message = f"ğŸ“° *{title}*\n\n{summary_text}\n\n[Äá»c bÃ i viáº¿t Ä‘áº§y Ä‘á»§]({link})"
                send_telegram_message(message)
                time.sleep(1) # Chá» 1 giÃ¢y Ä‘á»ƒ trÃ¡nh spam Telegram
        else:
            logger.warning("KhÃ´ng cÃ³ bÃ i bÃ¡o nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng.")

    finally:
        save_processed_links(processed_links)
        scraper.close()
        logger.info("--- HoÃ n táº¥t chu ká»³. ---")

if __name__ == "__main__":
    process_news()
