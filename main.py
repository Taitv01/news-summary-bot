#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import os
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Set, Optional
from dataclasses import dataclass

# Imports cho connection handling c·∫£i ti·∫øn
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from scraper import NewsScraper
from content_extractor import extract_content
from telegram_sender import send_telegram_message, escape_markdown_v2
from summarizer import summarize_with_gemini

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Constants t·ªëi ∆∞u
MAX_ARTICLES_PER_SOURCE = 8  # Gi·∫£m t·ª´ 10 xu·ªëng 8
MESSAGE_LIMIT = 4000
DATA_DIR = 'data'
PROCESSED_LINKS_FILE = os.path.join(DATA_DIR, 'processed_links.json')
MAX_CONCURRENT_REQUESTS = 2  # Gi·∫£m t·ª´ 3 xu·ªëng 2

@dataclass
class Article:
    """C·∫•u tr√∫c d·ªØ li·ªáu cho b√†i b√°o."""
    title: str
    link: str
    content: Optional[str] = None

def load_rss_sources():
    """T·∫£i c√°c ngu·ªìn RSS ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† c·∫≠p nh·∫≠t."""
    return {
        # B√°o ch√≠ ch√≠nh th·ªëng - Ngu·ªìn ·ªïn ƒë·ªãnh
        'VnExpress M·ªõi nh·∫•t': 'https://vnexpress.net/rss/tin-moi-nhat.rss',
        'VnExpress Kinh doanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'VnExpress Th·ªÉ thao': 'https://vnexpress.net/rss/the-thao.rss',
        'Tu·ªïi Tr·∫ª': 'https://tuoitre.vn/rss/tin-moi-nhat.rss',
        'Thanh Ni√™n': 'https://thanhnien.vn/rss/home.rss',
        'Ph√°p lu·∫≠t TP.HCM': 'https://plo.vn/rss/home.rss', # Ngu·ªìn n√†y v·∫´n ·ªïn ƒë·ªãnh
        
        # C√°c ngu·ªìn ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t URL
        'Zing News (Th·ªùi s·ª±)': 'https://znews.vn/rss/thoi-su.rss',
        'Lao ƒê·ªông': 'https://laodong.vn/rss/trang-chu.rss',
        'ICTnews (C√¥ng ngh·ªá)': 'https://ictnews.vietnamnet.vn/rss/cong-nghe.rss',
        'BBC Ti·∫øng Vi·ªát': 'http://feeds.bbci.co.uk/vietnamese/rss.xml',
        'VOV (Tin 24h)': 'https://vov.vn/rss/tin-24h-298.rss',
        'Vietnamplus': 'https://www.vietnamplus.vn/rss/trangchu.rss',
        'B√°o Ch√≠nh ph·ªß': 'https://baochinhphu.vn/rss/chinh-sach-moi.rss',
        'Nh√¢n D√¢n': 'https://nhandan.vn/api/rss/trang-chu'
    }
def create_robust_session():
    """T·∫°o session HTTP v·ªõi retry v√† pool management."""
    session = requests.Session()
    
    # C·∫•u h√¨nh retry strategy
    retry_strategy = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )
    
    adapter = HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=10,
        pool_maxsize=20,
        pool_block=False
    )
    
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def load_processed_links():
    """T·∫£i danh s√°ch c√°c link ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω."""
    if os.path.exists(PROCESSED_LINKS_FILE):
        try:
            with open(PROCESSED_LINKS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except json.JSONDecodeError:
            logger.warning(f"L·ªói ƒë·ªçc file {PROCESSED_LINKS_FILE}. B·∫Øt ƒë·∫ßu v·ªõi danh s√°ch r·ªóng.")
            return set()
    return set()

def save_processed_links(links):
    """L∆∞u danh s√°ch c√°c link ƒë√£ x·ª≠ l√Ω."""
    os.makedirs(DATA_DIR, exist_ok=True)
    with open(PROCESSED_LINKS_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)

def split_message(text: str, limit: int = MESSAGE_LIMIT) -> List[str]:
    """Chia m·ªôt ƒëo·∫°n vƒÉn b·∫£n d√†i th√†nh nhi·ªÅu ph·∫ßn nh·ªè h∆°n gi·ªõi h·∫°n."""
    if len(text) <= limit:
        return [text]
    
    parts = []
    while len(text) > 0:
        if len(text) <= limit:
            parts.append(text)
            break
        split_pos = text.rfind('\n', 0, limit)
        if split_pos == -1:
            split_pos = limit
        parts.append(text[:split_pos])
        text = text[split_pos:].lstrip()
    return parts

def fetch_rss_with_fallback(source_name: str, rss_url: str, session: requests.Session, max_retries: int = 3) -> List[Article]:
    """RSS parser m·∫°nh m·∫Ω v·ªõi error handling c·∫£i ti·∫øn."""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*',
        'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Cache-Control': 'no-cache'
    }
    
    for attempt in range(max_retries):
        try:
            logger.info(f"-> ƒêang l·∫•y t·ª´: {source_name} (l·∫ßn th·ª≠ {attempt + 1})")
            
            # B∆∞·ªõc 1: T·∫£i RSS v·ªõi session
            try:
                response = session.get(rss_url, headers=headers, timeout=15)
                response.raise_for_status()
                
                # Ki·ªÉm tra content type
                content_type = response.headers.get('content-type', '').lower()
                if 'html' in content_type and 'xml' not in content_type:
                    logger.warning(f"‚ö†Ô∏è {source_name} tr·∫£ v·ªÅ HTML thay v√¨ RSS")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    else:
                        return []
                        
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå L·ªói t·∫£i RSS t·ª´ {source_name}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    return []
            
            # B∆∞·ªõc 2: Parse RSS v·ªõi feedparser
            try:
                # L√†m s·∫°ch content tr∆∞·ªõc khi parse
                content = response.text
                
                # X·ª≠ l√Ω encoding n·∫øu c·∫ßn
                if response.encoding:
                    content = content.encode(response.encoding).decode('utf-8', errors='ignore')
                
                # Parse v·ªõi feedparser
                feed = feedparser.parse(content)
                
                # Ki·ªÉm tra l·ªói parse
                if feed.bozo:
                    logger.warning(f"‚ö†Ô∏è RSS parsing warning t·ª´ {source_name}: {feed.bozo_exception}")
                    
                    # V·∫´n ti·∫øp t·ª•c n·∫øu c√≥ entries
                    if not feed.entries:
                        if attempt < max_retries - 1:
                            time.sleep(2 ** attempt)
                            continue
                        else:
                            return []
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói parse RSS t·ª´ {source_name}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    return []
            
            # B∆∞·ªõc 3: Tr√≠ch xu·∫•t articles
            articles = []
            entries = feed.entries[:MAX_ARTICLES_PER_SOURCE]
            
            for entry in entries:
                try:
                    # Ki·ªÉm tra required fields
                    if not (hasattr(entry, 'link') and hasattr(entry, 'title')):
                        continue
                    
                    # Clean title v√† link
                    title = entry.title.strip()
                    link = entry.link.strip()
                    
                    # Validate URL
                    if not link.startswith(('http://', 'https://')):
                        continue
                    
                    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát trong title
                    title = title.replace('\n', ' ').replace('\r', ' ')
                    while '  ' in title:
                        title = title.replace('  ', ' ')
                    
                    articles.append(Article(title=title, link=link))
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω entry t·ª´ {source_name}: {e}")
                    continue
            
            if articles:
                logger.info(f"‚úÖ L·∫•y th√†nh c√¥ng {len(articles)} b√†i t·ª´ {source_name}")
                return articles
            else:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ b√†i b√°o h·ª£p l·ªá t·ª´ {source_name}")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh√¥ng mong mu·ªën t·ª´ {source_name} (l·∫ßn {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                return []
    
    return []

def process_single_article(article: Article, scraper: NewsScraper) -> Optional[Article]:
    """X·ª≠ l√Ω m·ªôt b√†i b√°o v·ªõi error handling c·∫£i ti·∫øn."""
    try:
        response = scraper.get_content_with_retry(article.link)
        if response and response.status_code == 200:
            content = extract_content(response.text, article.link)
            if content:
                # L√†m s·∫°ch content
                content = content.strip()
                if len(content) > 10:  # Ch·ªâ l·∫•y n·ªôi dung c√≥ √Ω nghƒ©a
                    article.content = f"TI√äU ƒê·ªÄ: {article.title}\nN·ªòI DUNG:\n{content}"
                    return article
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω b√†i b√°o {article.link}: {e}")
    return None

def send_health_report(successful_sources, failed_sources, total_articles):
    """G·ª≠i b√°o c√°o t√¨nh tr·∫°ng h·ªá th·ªëng."""
    try:
        if not successful_sources and not failed_sources:
            return
            
        total_sources = len(successful_sources) + len(failed_sources)
        success_rate = len(successful_sources) / total_sources * 100 if total_sources > 0 else 0
        
        # Ch·ªâ g·ª≠i b√°o c√°o khi c√≥ v·∫•n ƒë·ªÅ ho·∫∑c t·ª∑ l·ªá th√†nh c√¥ng th·∫•p
        if failed_sources or success_rate < 70:
            status_emoji = "‚úÖ" if success_rate >= 70 else "‚ö†Ô∏è" if success_rate >= 50 else "‚ùå"
            
            report = f"""{status_emoji} **B√°o c√°o News Bot**

üìä **Th·ªëng k√™:**
‚Ä¢ T·ª∑ l·ªá th√†nh c√¥ng: {success_rate:.1f}% ({len(successful_sources)}/{total_sources})
‚Ä¢ B√†i b√°o thu th·∫≠p: {total_articles}
‚Ä¢ Th·ªùi gian: {time.strftime('%Y-%m-%d %H:%M:%S')}

‚úÖ **Ngu·ªìn th√†nh c√¥ng:**
{', '.join(successful_sources[:5])}{'...' if len(successful_sources) > 5 else ''}

‚ùå **Ngu·ªìn th·∫•t b·∫°i:**
{', '.join(failed_sources[:5])}{'...' if len(failed_sources) > 5 else ''}"""
            
            send_telegram_message(report)
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói g·ª≠i b√°o c√°o: {e}")

def process_news():
    """Quy tr√¨nh x·ª≠ l√Ω tin t·ª©c ch√≠nh v·ªõi error handling c·∫£i ti·∫øn."""
    logger.info("--- Bot t√≥m t·∫Øt tin t·ª©c b·∫Øt ƒë·∫ßu ch·∫°y ---")
    
    # T·∫°o session chung cho RSS
    http_session = create_robust_session()
    scraper = NewsScraper()
    
    try:
        processed_links = load_processed_links()
        rss_sources = load_rss_sources()
        logger.info(f"ƒêang ki·ªÉm tra tin t·ª©c t·ª´ {len(rss_sources)} ngu·ªìn RSS...")
        
        # L·∫•y b√†i b√°o m·ªõi v·ªõi x·ª≠ l√Ω song song
        all_new_articles = []
        successful_sources = []
        failed_sources = []
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_source = {
                executor.submit(fetch_rss_with_fallback, source_name, rss_url, http_session): source_name
                for source_name, rss_url in rss_sources.items()
            }
            
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                try:
                    articles = future.result(timeout=30)  # Timeout cho m·ªói source
                    if articles:
                        # L·ªçc b√†i b√°o ch∆∞a x·ª≠ l√Ω
                        new_articles = [
                            article for article in articles 
                            if article.link not in processed_links
                        ]
                        all_new_articles.extend(new_articles)
                        successful_sources.append(source_name)
                        logger.info(f"‚úÖ {source_name}: {len(new_articles)} b√†i m·ªõi")
                    else:
                        failed_sources.append(source_name)
                        logger.warning(f"‚ö†Ô∏è {source_name}: Kh√¥ng c√≥ b√†i b√°o")
                except Exception as e:
                    failed_sources.append(source_name)
                    logger.error(f"‚ùå L·ªói x·ª≠ l√Ω ngu·ªìn {source_name}: {e}")
        
        # B√°o c√°o chi ti·∫øt
        total_sources = len(rss_sources)
        success_rate = len(successful_sources) / total_sources * 100
        
        logger.info(f"üìä K·∫øt qu·∫£ thu th·∫≠p RSS:")
        logger.info(f"  ‚úÖ Th√†nh c√¥ng: {len(successful_sources)}/{total_sources} ngu·ªìn ({success_rate:.1f}%)")
        logger.info(f"  ‚ùå Th·∫•t b·∫°i: {len(failed_sources)} ngu·ªìn")
        logger.info(f"  üì∞ T·ªïng b√†i b√°o m·ªõi: {len(all_new_articles)}")
        
        if failed_sources:
            logger.warning(f"  üî∏ Ngu·ªìn th·∫•t b·∫°i: {', '.join(failed_sources)}")
        
        # G·ª≠i b√°o c√°o health
        send_health_report(successful_sources, failed_sources, len(all_new_articles))
        
        if not all_new_articles:
            logger.info("Kh√¥ng c√≥ b√†i b√°o m·ªõi.")
            return

        # X·ª≠ l√Ω n·ªôi dung b√†i b√°o
        logger.info("B·∫Øt ƒë·∫ßu thu th·∫≠p n·ªôi dung...")
        
        successful_articles = []
        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
            future_to_article = {
                executor.submit(process_single_article, article, scraper): article 
                for article in all_new_articles
            }
            
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    result = future.result(timeout=60)  # Timeout cho m·ªói article
                    if result and result.content:
                        successful_articles.append(result)
                        processed_links.add(result.link)
                except Exception as e:
                    logger.error(f"‚ùå L·ªói x·ª≠ l√Ω b√†i b√°o {article.link}: {e}")

        if not successful_articles:
            logger.warning("Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c n·ªôi dung t·ª´ b·∫•t k·ª≥ b√†i b√°o m·ªõi n√†o.")
            return

        logger.info(f"ƒê√£ thu th·∫≠p th√†nh c√¥ng {len(successful_articles)} b√†i b√°o")
        
        # T·∫°o t√≥m t·∫Øt c√¢n b·∫±ng t·ª´ nhi·ªÅu ngu·ªìn
        combined_text = "\n\n---H·∫æT B√ÄI B√ÅO---\n\n".join(
            article.content for article in successful_articles
        )
        
        logger.info("ƒêang t·∫°o t√≥m t·∫Øt v·ªõi AI...")
        final_summary = summarize_with_gemini(combined_text)
        
        # Th√™m th√¥ng tin th·ªëng k√™ v√†o t√≥m t·∫Øt
        source_info = f"\n\nüìä **Th·ªëng k√™ ngu·ªìn tin:**\n‚úÖ {len(successful_sources)} ngu·ªìn th√†nh c√¥ng\nüì∞ {len(successful_articles)} b√†i b√°o ƒë∆∞·ª£c x·ª≠ l√Ω\nüïê {time.strftime('%H:%M %d/%m/%Y')}"
        
        logger.info("G·ª≠i tin nh·∫Øn...")
        
        # X·ª≠ l√Ω v√† g·ª≠i tin nh·∫Øn
        escaped_content = escape_markdown_v2(final_summary)
        escaped_source_info = escape_markdown_v2(source_info)
        full_message_body = f"üì∞ **B·∫¢N TIN T·ªîNG H·ª¢P H√îM NAY**\n\n{escaped_content}{escaped_source_info}"
        
        message_chunks = split_message(full_message_body, MESSAGE_LIMIT)
        
        for i, chunk in enumerate(message_chunks):
            message_to_send = chunk
            if len(message_chunks) > 1:
                note = escape_markdown_v2(f"\n\n(Ph·∫ßn {i+1}/{len(message_chunks)})")
                message_to_send += note

            send_telegram_message(message_to_send)
            time.sleep(1)

    except Exception as e:
        logger.error(f"‚ùå L·ªói ch√≠nh trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
        # G·ª≠i th√¥ng b√°o l·ªói
        try:
            error_msg = f"üö® **L·ªñI BOT TIN T·ª®C**\n\n{str(e)[:500]}..."
            send_telegram_message(error_msg)
        except:
            pass
    finally:
        # Cleanup resources
        try:
            if 'http_session' in locals():
                http_session.close()
            if 'scraper' in locals():
                scraper.close()
        except:
            pass
        
        save_processed_links(processed_links)
        logger.info("--- Ho√†n t·∫•t chu k·ª≥. ---")

if __name__ == "__main__":
    process_news()
