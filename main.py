#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import os
import logging
import time
from scraper import NewsScraper
from content_extractor import extract_content
from telegram_sender import send_telegram_message, escape_markdown_v2
from summarizer import summarize_with_gemini

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_rss_sources():
    """T·∫£i c√°c ngu·ªìn RSS t·ª´ c·∫•u h√¨nh."""
    return {
        'VnExpress M·ªõi nh·∫•t': 'https://vnexpress.net/rss/tin-moi-nhat.rss',
        'VnExpress Kinh doanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'Vietstock Ch·ª©ng kho√°n': 'https://vietstock.vn/rss/chung-khoan.rss',
        'Lao ƒê·ªông': 'https://laodong.vn/rss/tin-moi-nhat.rss'
    }

def load_processed_links():
    """T·∫£i danh s√°ch c√°c link ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω."""
    file_path = 'data/processed_links.json'
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except json.JSONDecodeError:
            logger.warning(f"L·ªói ƒë·ªçc file {file_path}. B·∫Øt ƒë·∫ßu v·ªõi danh s√°ch r·ªóng.")
            return set()
    return set()

def save_processed_links(links):
    """L∆∞u danh s√°ch c√°c link ƒë√£ x·ª≠ l√Ω."""
    os.makedirs('data', exist_ok=True)
    with open('data/processed_links.json', 'w', encoding='utf-8') as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)

def split_message(text: str, limit: int = 4000) -> list[str]:
    """Chia m·ªôt ƒëo·∫°n vƒÉn b·∫£n d√†i th√†nh nhi·ªÅu ph·∫ßn nh·ªè h∆°n gi·ªõi h·∫°n."""
    if len(text) <= limit:
        return [text]
    parts = []
    while len(text) > 0:
        if len(text) <= limit:
            parts.append(text)
            break
        split_pos = text.rfind('\n', 0, limit)
        if split_pos == -1:
            split_pos = limit
        parts.append(text[:split_pos])
        text = text[split_pos:].lstrip()
    return parts

def process_news():
    """Quy tr√¨nh x·ª≠ l√Ω tin t·ª©c ch√≠nh."""
    logger.info("--- Bot t√≥m t·∫Øt tin t·ª©c b·∫Øt ƒë·∫ßu ch·∫°y ---")
    scraper = NewsScraper()
    
    try:
        processed_links = load_processed_links()
        rss_sources = load_rss_sources()
        logger.info("ƒêang ki·ªÉm tra tin t·ª©c t·ª´ t·∫•t c·∫£ c√°c ngu·ªìn RSS...")
        
        new_articles = []
        for source_name, rss_url in rss_sources.items():
            logger.info(f"-> ƒêang l·∫•y t·ª´: {source_name}")
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:10]: # Gi·ªõi h·∫°n 10 tin m·ªói ngu·ªìn
                if entry.link not in processed_links:
                    new_articles.append({'title': entry.title, 'link': entry.link})
        
        if not new_articles:
            logger.info("Kh√¥ng c√≥ b√†i b√°o m·ªõi.")
            return

        logger.info(f"Ph√°t hi·ªán {len(new_articles)} b√†i b√°o m·ªõi. B·∫Øt ƒë·∫ßu thu th·∫≠p n·ªôi dung...")
        
        all_articles_content = []
        for article in new_articles:
            response = scraper.get_content_with_retry(article['link'])
            if response and response.status_code == 200:
                content = extract_content(response.text, article['link'])
                if content:
                    full_content = f"TI√äU ƒê·ªÄ: {article['title']}\nN·ªòI DUNG:\n{content}"
                    all_articles_content.append(full_content)
                    processed_links.add(article['link'])

        if not all_articles_content:
            logger.warning("Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c n·ªôi dung t·ª´ b·∫•t k·ª≥ b√†i b√°o m·ªõi n√†o.")
            return

        logger.info(f"ƒê√£ thu th·∫≠p {len(all_articles_content)} b√†i b√°o. G·ªôp l·∫°i v√† g·ª≠i ƒëi t√≥m t·∫Øt...")
        combined_text = "\n\n---H·∫æT B√ÄI B√ÅO---\n\n".join(all_articles_content)
        
        final_summary = summarize_with_gemini(combined_text)
        
        logger.info("ƒê√£ nh·∫≠n t√≥m t·∫Øt t·ª´ AI. Chu·∫©n b·ªã chia v√† g·ª≠i tin nh·∫Øn...")
        
        # B∆Ø·ªöC 1: X·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát CH·ªà cho n·ªôi dung do AI t·∫°o ra.
        escaped_content = escape_markdown_v2(final_summary)
        
        # B∆Ø·ªöC 2: Th√™m ti√™u ƒë·ªÅ (ch·ª©a k√Ω t·ª± Markdown *...*) SAU KHI ƒë√£ x·ª≠ l√Ω.
        full_message_body = f"üì∞ *B·∫¢N TIN T·ªîNG H·ª¢P H√îM NAY*\n\n{escaped_content}"
        
        # B∆Ø·ªöC 3: Chia tin nh·∫Øn th√†nh c√°c ph·∫ßn nh·ªè.
        message_chunks = split_message(full_message_body, 4000)
        
        for i, chunk in enumerate(message_chunks):
            message_to_send = chunk
            # Th√™m ghi ch√∫ (Ph·∫ßn x/y) n·∫øu c√≥ nhi·ªÅu h∆°n 1 ph·∫ßn
            if len(message_chunks) > 1:
                # D√πng k√Ω t·ª± escape cho d·∫•u ngo·∫∑c ƒë∆°n
                note = escape_markdown_v2(f"\n\n(Ph·∫ßn {i+1}/{len(message_chunks)})")
                message_to_send += note

            send_telegram_message(message_to_send)
            time.sleep(1)

    finally:
        save_processed_links(processed_links)
        scraper.close()
        logger.info("--- Ho√†n t·∫•t chu k·ª≥. ---")

if __name__ == "__main__":
    process_news()
