#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import os
import logging
import time
from scraper import NewsScraper
from content_extractor import extract_content
from telegram_sender import send_telegram_message
from summarizer import summarize_with_gemini

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_rss_sources():
    """T·∫£i c√°c ngu·ªìn RSS t·ª´ c·∫•u h√¨nh."""
    return {
        'VnExpress M·ªõi nh·∫•t': 'https://vnexpress.net/rss/tin-moi-nhat.rss',
        'VnExpress Kinh doanh': 'https://vnexpress.net/rss/kinh-doanh.rss',
        'Vietstock Ch·ª©ng kho√°n': 'https://vietstock.vn/rss/chung-khoan.rss',
        'Lao ƒê·ªông': 'https://laodong.vn/rss/tin-moi-nhat.rss'
    }

def load_processed_links():
    """T·∫£i danh s√°ch c√°c link ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω."""
    file_path = 'data/processed_links.json'
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except json.JSONDecodeError:
            logger.warning(f"L·ªói ƒë·ªçc file {file_path}. B·∫Øt ƒë·∫ßu v·ªõi danh s√°ch r·ªóng.")
            return set()
    return set()

def save_processed_links(links):
    """L∆∞u danh s√°ch c√°c link ƒë√£ x·ª≠ l√Ω."""
    os.makedirs('data', exist_ok=True)
    with open('data/processed_links.json', 'w', encoding='utf-8') as f:
        json.dump(list(links), f, ensure_ascii=False, indent=2)

def process_news():
    """Quy tr√¨nh x·ª≠ l√Ω tin t·ª©c ch√≠nh."""
    logger.info("--- Bot t√≥m t·∫Øt tin t·ª©c b·∫Øt ƒë·∫ßu ch·∫°y ---")
    scraper = NewsScraper()
    
    try:
        processed_links = load_processed_links()
        logger.info(f"ƒê√£ t·∫£i {len(processed_links)} links ƒë√£ x·ª≠ l√Ω.")
        
        rss_sources = load_rss_sources()
        logger.info("ƒêang ki·ªÉm tra tin t·ª©c t·ª´ t·∫•t c·∫£ c√°c ngu·ªìn RSS...")
        
        new_articles = []
        for source_name, rss_url in rss_sources.items():
            logger.info(f"-> ƒêang l·∫•y t·ª´: {source_name}")
            feed = feedparser.parse(rss_url)
            for entry in feed.entries:
                if entry.link not in processed_links:
                    new_articles.append({'title': entry.title, 'link': entry.link})
        
        if not new_articles:
            logger.info("Kh√¥ng c√≥ b√†i b√°o m·ªõi.")
            return

        logger.info(f"Ph√°t hi·ªán {len(new_articles)} b√†i b√°o m·ªõi. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        # --- V√íNG L·∫∂P 1: THU TH·∫¨P V√Ä T√ìM T·∫ÆT ---
        # V√≤ng l·∫∑p n√†y ch·ªâ ƒë·ªÉ l·∫•y n·ªôi dung v√† t√≥m t·∫Øt, l∆∞u v√†o m·ªôt danh s√°ch m·ªõi.
        articles_to_send = []
        for article in new_articles:
            logger.info(f"ƒêang x·ª≠ l√Ω: {article['title'][:40]}...")
            response = scraper.get_content_with_retry(article['link'])
            
            if response and response.status_code == 200:
                content = extract_content(response.text, article['link'])
                if content:
                    logger.info("-> L·∫•y n·ªôi dung th√†nh c√¥ng, ƒëang g·ª≠i ƒëi t√≥m t·∫Øt...")
                    summary = summarize_with_gemini(content)
                    article['summary'] = summary
                    articles_to_send.append(article)
                    processed_links.add(article['link']) # ƒê√°nh d·∫•u ƒë√£ x·ª≠ l√Ω
                else:
                    logger.warning("-> Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c n·ªôi dung.")
            else:
                logger.warning("-> Kh√¥ng t·∫£i ƒë∆∞·ª£c trang.")

        # --- V√íNG L·∫∂P 2: G·ª¨I K·∫æT QU·∫¢ T·ªöI TELEGRAM ---
        # V√≤ng l·∫∑p n√†y ch·ªâ ƒë·ªÉ g·ª≠i tin nh·∫Øn, gi√∫p qu·∫£n l√Ω logic d·ªÖ d√†ng h∆°n.
        if articles_to_send:
            logger.info(f"ƒê√£ x·ª≠ l√Ω xong. Chu·∫©n b·ªã g·ª≠i {len(articles_to_send)} tin nh·∫Øn ƒë·∫øn Telegram.")
            for article in articles_to_send:
                title = article['title'].replace('-', r'\-').replace('.', r'\.').replace('!', r'\!').replace('(', r'\(').replace(')', r'\)')
                link = article['link'].replace('-', r'\-').replace('.', r'\.')
                summary_text = article.get('summary', 'Kh√¥ng c√≥ t√≥m t·∫Øt').replace('-', r'\-').replace('.', r'\.')
                
                message = f"üì∞ *{title}*\n\n{summary_text}\n\n[ƒê·ªçc b√†i vi·∫øt ƒë·∫ßy ƒë·ªß]({link})"
                send_telegram_message(message)
                time.sleep(1) # Ch·ªù 1 gi√¢y ƒë·ªÉ tr√°nh spam Telegram
        else:
            logger.warning("Kh√¥ng c√≥ b√†i b√°o n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng.")

    finally:
        save_processed_links(processed_links)
        scraper.close()
        logger.info("--- Ho√†n t·∫•t chu k·ª≥. ---")

if __name__ == "__main__":
    process_news()
