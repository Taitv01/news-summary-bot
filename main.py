#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import json
import os
import logging
import time
from scraper import NewsScraper
from content_extractor import extract_content
from telegram_sender import send_telegram_message, escape_markdown_v2
from summarizer import summarize_with_gemini

# (CÃ¡c hÃ m cáº¥u hÃ¬nh logging, load_rss_sources, etc. giá»¯ nguyÃªn nhÆ° cÅ©)
# ...
# ... (Giá»¯ nguyÃªn cÃ¡c hÃ m tá»« load_rss_sources Ä‘áº¿n save_processed_links)

def split_message(text: str, limit: int = 4000) -> list[str]:
    """Chia má»™t Ä‘oáº¡n vÄƒn báº£n dÃ i thÃ nh nhiá»u pháº§n nhá» hÆ¡n giá»›i háº¡n."""
    if len(text) <= limit:
        return [text]

    parts = []
    while len(text) > 0:
        if len(text) <= limit:
            parts.append(text)
            break
        
        # TÃ¬m vá»‹ trÃ­ ngáº¯t dÃ²ng gáº§n nháº¥t tá»« cuá»‘i
        split_pos = text.rfind('\n', 0, limit)
        if split_pos == -1: # KhÃ´ng tÃ¬m tháº¥y ngáº¯t dÃ²ng, cáº¯t táº¡i giá»›i háº¡n
            split_pos = limit
        
        parts.append(text[:split_pos])
        text = text[split_pos:].lstrip()
        
    return parts

def process_news():
    """Quy trÃ¬nh xá»­ lÃ½ tin tá»©c chÃ­nh."""
    logger.info("--- Bot tÃ³m táº¯t tin tá»©c báº¯t Ä‘áº§u cháº¡y ---")
    scraper = NewsScraper()
    
    try:
        processed_links = load_processed_links()
        # ... (Pháº§n code láº¥y new_articles tá»« RSS giá»¯ nguyÃªn)

        if not new_articles:
            logger.info("KhÃ´ng cÃ³ bÃ i bÃ¡o má»›i.")
            return

        logger.info(f"PhÃ¡t hiá»‡n {len(new_articles)} bÃ i bÃ¡o má»›i. Báº¯t Ä‘áº§u thu tháº­p ná»™i dung...")
        
        # --- BÆ¯á»šC 1: THU THáº¬P Táº¤T Cáº¢ Ná»˜I DUNG ---
        all_articles_content = []
        for article in new_articles:
            response = scraper.get_content_with_retry(article['link'])
            if response and response.status_code == 200:
                content = extract_content(response.text, article['link'])
                if content:
                    # Gá»™p tiÃªu Ä‘á» vÃ  ná»™i dung láº¡i
                    full_content = f"TIÃŠU Äá»€: {article['title']}\nNá»˜I DUNG:\n{content}"
                    all_articles_content.append(full_content)
                    processed_links.add(article['link'])

        if not all_articles_content:
            logger.warning("KhÃ´ng thu tháº­p Ä‘Æ°á»£c ná»™i dung tá»« báº¥t ká»³ bÃ i bÃ¡o má»›i nÃ o.")
            return

        # --- BÆ¯á»šC 2: Gá»˜P VÃ€ TÃ“M Táº®T TRONG 1 Láº¦N Gá»ŒI API ---
        logger.info(f"ÄÃ£ thu tháº­p {len(all_articles_content)} bÃ i bÃ¡o. Gá»™p láº¡i vÃ  gá»­i Ä‘i tÃ³m táº¯t...")
        combined_text = "\n\n---Háº¾T BÃ€I BÃO---\n\n".join(all_articles_content)
        
        final_summary = summarize_with_gemini(combined_text)
        
        # --- BÆ¯á»šC 3: CHIA TIN NHáº®N VÃ€ Gá»¬I Äáº¾N TELEGRAM ---
        logger.info("ÄÃ£ nháº­n tÃ³m táº¯t tá»« AI. Chuáº©n bá»‹ chia vÃ  gá»­i tin nháº¯n...")
        
        # Escape toÃ n bá»™ báº£n tÃ³m táº¯t má»™t láº§n
        escaped_summary = escape_markdown_v2(final_summary)
        
        message_chunks = split_message(escaped_summary, 4000)
        
        for i, chunk in enumerate(message_chunks):
            # ThÃªm tiÃªu Ä‘á» cho cÃ¡c pháº§n tin nháº¯n
            header = f"ğŸ“° *Báº¢N TIN Tá»”NG Há»¢P (Pháº§n {i+1}/{len(message_chunks)})*\n\n"
            full_message = header + chunk
            
            send_telegram_message(full_message)
            time.sleep(1)

    finally:
        save_processed_links(processed_links)
        scraper.close()
        logger.info("--- HoÃ n táº¥t chu ká»³. ---")

if __name__ == "__main__":
    process_news()
